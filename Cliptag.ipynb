{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDAOVA8dtmR-",
        "outputId": "ca96e5b3-5566-4264-d4e4-71b1e7ef8c49"
      },
      "outputs": [],
      "source": [
        "%pip install pillow\n",
        "%pip install torch\n",
        "%pip install open_clip_torch\n",
        "%pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fxb-O-X5VJK",
        "outputId": "92f2ab74-a053-4980-e01d-ce27c80e5e62"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import open_clip\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Use the GPU (CUDA) as the device\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    # Use the CPU as the device\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# Load the CLIP model and tokenizer\n",
        "# model, preprocess = open_clip.load('ViT-B/32')\n",
        "\n",
        "print(\"Loading model...\")\n",
        "\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "    'ViT-B-32', \n",
        "    pretrained='laion2b_s34b_b79k',\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u9SdOcnkuXHw"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def load_keywords_from_urls(url_list):\n",
        "    keyword_list = []\n",
        "    for url in url_list:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            # Split the content by lines and strip leading/trailing whitespace\n",
        "            keywords = [line.strip() for line in response.text.splitlines()]\n",
        "            # Add keywords to the master keyword_list\n",
        "            keyword_list.extend(keywords)\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error loading keywords from URL ({url}): {e}\")\n",
        "    print(f\"Loaded {len(keyword_list)} keywords...\")\n",
        "    return keyword_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BkW3JtzgDBu1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def load_keywords_from_files(directory):\n",
        "    keyword_list = []\n",
        "    for filename in os.listdir(directory):\n",
        "        # Check if the file has a .txt extension\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            with open(file_path, 'r') as file:\n",
        "                # Read lines and strip leading/trailing whitespace\n",
        "                keywords = [line.strip() for line in file]\n",
        "                # Add keywords to the master keyword_list\n",
        "                keyword_list.extend(keywords)\n",
        "    print(f\"Loaded {len(keyword_list)} keywords...\")\n",
        "    return keyword_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nV3Dh6CdvCVh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import open_clip\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "def generate_keywords_for_image(image_url, candidate_keywords, text_features, top_k=5, batch_size=100):\n",
        "\n",
        "  try:\n",
        "    # Load the CLIP model and tokenizer\n",
        "    print(\"Retrieving image from URL...\")\n",
        "\n",
        "    response = requests.get(image_url)\n",
        "    response.raise_for_status()\n",
        "    image = Image.open(BytesIO(response.content))\n",
        "\n",
        "    # Open the image and preprocess it\n",
        "    # image = Image.open(image_path)\n",
        "\n",
        "    print(\"Preprocessing image...\")\n",
        "    image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Encode the image and keywords\n",
        "    print(\"Encoding image and keywords...\")\n",
        "    with torch.no_grad():\n",
        "        print(\"Encoding image...\")\n",
        "        image_features = model.encode_image(image_tensor)\n",
        "\n",
        "    # Calculate the similarity scores\n",
        "    print(\"Calculating similarity scores...\")\n",
        "    similarities = torch.matmul(image_features, text_features.T)\n",
        "\n",
        "    # Get the top K keywords based on the similarity scores\n",
        "    print(\"Getting top keywords...\")\n",
        "    top_indices = torch.topk(similarities, top_k, dim=-1).indices.squeeze(0)\n",
        "    top_keywords = [candidate_keywords[i] for i in top_indices]\n",
        "\n",
        "    return top_keywords\n",
        "\n",
        "  except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error loading image from URL: {e}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "in2xnpSI9X77"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def encode_keywords(candidate_keywords, batch_size=1000):\n",
        "    # Tokenize and encode the candidate keywords in batches\n",
        "    print(\"Tokenizing and encoding candidate keywords...\")\n",
        "    text_features = []\n",
        "    for i in range(0, len(candidate_keywords), batch_size):\n",
        "        print(f\"Batch {i+1}...\")\n",
        "        batch_keywords = candidate_keywords[i:i + batch_size]\n",
        "        batch_tokens = open_clip.tokenize(batch_keywords).to(device)\n",
        "        with torch.no_grad():\n",
        "            batch_features = model.encode_text(batch_tokens)\n",
        "        text_features.append(batch_features)\n",
        "    # Concatenate the encoded features from all batches\n",
        "    text_features = torch.cat(text_features, dim=0)\n",
        "\n",
        "    return text_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install h5py\n",
        "%pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pSgE68w04WQC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "def save_features_to_hdf5(keywords, features, file_path):\n",
        "    # Convert the keywords to a NumPy array of strings\n",
        "    keywords_array = np.array(keywords, dtype='S')\n",
        "\n",
        "    # Open the HDF5 file for writing\n",
        "    with h5py.File(file_path, 'w') as hf:\n",
        "        # Create and write the 'keywords' dataset\n",
        "        hf.create_dataset('keywords', data=keywords_array)\n",
        "        # Create and write the 'features' dataset\n",
        "        hf.create_dataset('features', data=features)\n",
        "\n",
        "def encode_and_save_keywords(candidate_keywords, feature_file_path, batch_size=1000, overwrite=False):\n",
        "    text_features = encode_keywords(candidate_keywords, batch_size)\n",
        "\n",
        "  \n",
        "    if os.path.exists(feature_file_path) and not overwrite:\n",
        "      # Append new features to the existing features\n",
        "      existing_features = torch.load(feature_file_path)\n",
        "      text_features = torch.cat([existing_features, text_features], dim=0)\n",
        "    \n",
        "    # Save the features to a file\n",
        "    torch.save(text_features, feature_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNS9lT5r4yhW"
      },
      "outputs": [],
      "source": [
        "# keyword_urls = [\n",
        "#     'https://raw.githubusercontent.com/pharmapsychotic/clip-interrogator/main/clip_interrogator/data/flavors.txt',\n",
        "#     'https://raw.githubusercontent.com/pharmapsychotic/clip-interrogator/main/clip_interrogator/data/mediums.txt',\n",
        "#     'https://raw.githubusercontent.com/pharmapsychotic/clip-interrogator/main/clip_interrogator/data/movements.txt'\n",
        "# ]\n",
        "# candidate_keywords = load_keywords_from_urls(keyword_urls)\n",
        "# print(f\"Loaded {len(candidate_keywords)} keywords\")\n",
        "\n",
        "# encode_and_save_keywords(candidate_keywords, \"keyword_features.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ1tJyL0-8qQ",
        "outputId": "1313dc44-baba-4d49-c1ee-ebf6b0c351f6"
      },
      "outputs": [],
      "source": [
        "color_keywords = load_keywords_from_files(\"features/test\")\n",
        "encode_and_save_keywords(color_keywords, \"keyword_features.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC-mxDJE7Cl_",
        "outputId": "d330bc3f-abb2-4eae-e188-a1253e3f27b1"
      },
      "outputs": [],
      "source": [
        "image_url = 'https://images.unsplash.com/photo-1682027888746-25b1af7bd47f?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1350&q=80'\n",
        "text_features = torch.load(\"keyword_features.pt\")\n",
        "candidate_keywords = load_keywords_from_files(\"features\")\n",
        "\n",
        "print(f\"Loaded {text_features.size()} features\")\n",
        "img_keywords = generate_keywords_for_image(image_url, candidate_keywords, text_features, top_k=20)\n",
        "\n",
        "print(img_keywords)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
